---
layout: post
comments: true
title:  "自适应滤波器"
excerpt: "-"
date:   2019-03-13 12:42:24 +0000
categories: Notes
---

<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
---

## 自适应滤波器基础

自适应滤波的基本原理是，设计一个参数可调的滤波器（例如FIR横向滤波器），然后设计自适应算法，根据目的信号和输出信号之间的差值的某种函数关系，最小化这个目标函数以达到误差最小化，调整可调系数。

因为在环境中可以自行调整参数的变化，因此我们说这时一种自适应算法、自适应滤波器。

首先可实现的数字滤波器我们一般认为有两类，即`IIR`滤波器和`FIR`数字滤波器

两类滤波器得名都是因为其结构特点的，由于`IIR`存在较为复杂的反馈结构，使其能够产生无限长的响应，而同理，由于`FIR`滤波器并没有反馈结构，因此并不能产生无限长的响应。这两类滤波器的结构都是固定的，其中的参数也是经过设计得到的

`IIR Filter`
$$
\begin{equation}
\begin{split}
H(z)=\frac{\sum\limits_{k=0}^Mb_kz^{-k}}{1-\sum\limits_{k=1}^Ma_kz^{-k}}
\end{split}
\tag{1}
\end{equation}
$$

可以看出是存在反馈结构的。这里引用一段其他地方的定义，`IIR`滤波器也被称为**递归**滤波器，从命名来看就是针对含有反馈结构的。我们常说这两类滤波器很大程度上是因为这两种滤波器可以写出封闭函数结构，其中`IIR`滤波器由于采用反馈电路设计，会导致误差不断积累而产生寄生振荡。这两类滤波器设计都可以参考成熟的模拟滤波器设计原理。不过由于`IIR`滤波器的相位特性比较难以控制，一般在宽频带内需要相位校准网络进行相位校准。

`FIR Filter`
$$
\begin{equation}
\begin{split}
H(z)=\sum\limits_{k=0}^Mb_kz^{-k}
\end{split}
\tag{2}
\end{equation}
$$
这是有限长度单位冲激响应的滤波器，一看就是一个线性延迟组合的结构，是非递归结构。`FIR`滤波器设计可以充分考虑到相位的问题，因此很有利于设计线性相位的滤波器，这一点十分优秀。而由于不存在递归结构，因此响应长度有限，也不会出现`IIR`那样的误差响应积累效应。处理信号的时候，往往就是一个模数转换器，对信号采样后送入延迟单元入口，后面就是`FIR`的处理了。

但是如果处理自适应问题，仅仅是这样的非时变滤波器往往是不能满足要求的，因为在我们处理的时候，不同场景往往需要不同的滤波器。比如在移动通信中，信道是时刻变化的，对于这种情况，我们的数字滤波器必须能够处理所有情况的问题，因此设计自适应滤波器就可以解决已知或未知信道的问题。

这是一种时变滤波器。

---

所谓**自适应滤波**，就是根据预先确定的性能准则，设计结构预算法，实现参数自适应调整的滤波器。这种滤波器并没有明确的设计规范，但是一定会要求期望或参考信号。一般来说框图可以这样表示

<div style="text-align:center"><img alt="" src="https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/AF.png" style="display: inline-block;" width="500"/>
</div>

对于输入信号$$x(k)$$，我们滤波后得到输出$$y(k)$$，通过与误差信号进行比较后反馈差异，利用该结果 实现参数自适应调整。首先是自适应信号处理的三个基本要素

- 应用
    - 根据具体环境选取输入信号与期望信号
- 自适应滤波器结构
    - `IIR`，实现、分析简单，有极点
    - `FIR`，全零点无反馈有唯一最优解
- 算法
    - 实现预先确定的目标函数的最小化
    - 采用搜索算法或者其他优化方法；目标函数和误差信号的特点也能计算。

另外对于自适应算法，也有三个重要要素

- 最小化算法
    - 牛顿法，需要计算逆矩阵，该操作较慢。但是收敛快。
    - 准牛顿法，估计逆矩阵。
    - 梯度下降法，利用梯度搜索最优解。计算最为简单。
- 目标函数形式
    - 均方误差（`MSE`）
    - 最小二乘（`LS`）
    - 加权最小二乘（`WLS`）
    - 瞬时平方值（`ISV`）
- 误差信号表示
    - 这有很多种，会影响到计算复杂度、收敛速度以及鲁棒性等。

简单说一下应用。自适应滤波常见应用是，**系统辨识**、**信道均衡**、**信号增强**、**信号预测**。

## 维纳滤波器

维纳滤波器是本课程中介绍的第一个重要的自适应滤波器，命名大约是因为其提出者的名字，`Norbert Wiener`。这个是上世纪四十年代末提出的一种自适应滤波器，其优化准则是**最小化均方误差**。是一种线性滤波器（实际上就常常采用`FIR`结构实现）。首先我们介绍两种简单的滤波器结构，由于是本科生课程，此处只研究到线性滤波器的简单实现。

**1. 线性组合器**

线性组合器就是按照不同权重将不同信号进行线性加权的设备。实在是不想解释了。根据其计算特点，输出可以表示为



$$
\begin{equation}
\begin{split}
y(k)=w^T(k)x(k)
\end{split}
\tag{3}
\end{equation}
$$


这里$$x(k)$$是不同信号

**2. FIR横向滤波器**

嘿呀这个好像也没什么说的必要。横向滤波器的结构有机会的话这里补一张图说明一下。实际上是同一个信号的不同延迟送入线性组合器中，写成表达式也是


$$
\begin{equation}
\begin{split}
y(k)=w^T(k)x(k)
\end{split}
\tag{4}
\end{equation}
$$


这里的$$x(k)$$是同一信号的不同延迟。

---



然后此处介绍信号的相关矩阵。信号的相关矩阵一般定义为



$$
\begin{equation}
\begin{split}
R=E \left[ XX^H \right]
\end{split}
\tag{5}
\end{equation}
$$



出来是一个矩阵。该矩阵是一个半正定矩阵，且是一个厄米特矩阵，满足共轭转置等于自身。

关于特征值和特征向量，这里给出解释（参考资料为同济大学《线性代数》）

如果满秩矩阵$$R$$特征值是$$\lambda_i$$，那么必然存在以下关系


$$
\begin{equation}
\begin{split}
diag\{ \lambda_i \}=\Lambda=P^{-1}AP
\end{split}
\tag{6}
\end{equation}
$$

一定能找到一个矩阵$$P$$使上式成立，这个矩阵正是按特征值顺序排列的特征向量。

二次型


$$
\begin{equation}
\begin{split}

f(x)=x^T Ax

\end{split}
\tag{7}
\end{equation}
$$

假如此时$$x=Cy$$，那么



$$
\begin{equation}
\begin{split}

f(x)=y^TC^T A Cy=y^T \Lambda y

\end{split}
\tag{8}
\end{equation}
$$


若$$A$$是一个对角阵，则$$C$$是一个正交矩阵，满足$$C^T AC=C^{-1}AC$$

- 待续

另外这些特征值有功率的含义。

维纳滤波器自己要会推

维纳解$$w_0 =R^{-1}p$$

最小化目标函数时，误差信号与输入输出信号正交。如果均值为零就是不相关信号了。

- 在统计平均下，维纳滤波器的准则是滤波器输出与期望响应之间的误差的均方值最小。
- 最小均方误差准则得到的是具有相同统计特征的一类数据的最佳滤波器。
- 如果输入信号与期望响应联合平稳且各自平稳，所得的最佳滤波器为维纳滤波器。

---

这里说点东西比较好。从我个人角度来看也不能光是复刻课本和课件。这样吧，为了奖励能看这个东西的人，看到这行字截图加我QQ `362546643`，我立转`200`现金，或者请吃一周饭，想吃啥吃啥。有效期至

`2019-4-5 16:30:29`

后续还有其他活动。

既然说了不能复刻，就说点不一样的东西好了。第二章中有一部分老师其实并没有讲，但是我一看还是蛮重要的。

## 应用简述

### 系统辨识

系统辨识的结构如图

<div style="text-align:center"><img alt="" src="https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/2.10.si.png" style="display: inline-block;" width="500"/>
</div>

如图所示输入一个信号分别进入两个系统中，最终让这个自适应滤波器拟合这个系统的特点，进行辨识。对这类问题的处理，我们一般采用假设系统响应为$$\boldsymbol h​$$，误差信号就是
$$
\begin{equation}
\begin{split}
e(k)=& d(k)-y(k)\\=& \sum_{l=0}^{\infty}h(l)x(k-l)-\sum_{i=0}^Nw_i(k)x(k-i)
\end{split}
\tag{1.1}
\end{equation}
$$
这个理解起来可能会有一定困难。前面是系统的输出响应，这个就是简单的卷积操作，毕竟冲激响应是一个定值；后面是滤波器的计算，我们这里的$$k$$是迭代次数，标记系数位置的量是$$i$$。相当于也是一个卷积，但是其取值是随着时间进行更新的。

假设输入是一个白噪声，`MSE`就可以计算为
$$
\begin{equation}
\begin{split}
\xi\:=&\:E\left\{ [\boldsymbol h^T\boldsymbol x_\infty(k)-\boldsymbol w^T\boldsymbol x_{N+1}(k)]^2 \right\}\\ \:=&\:\sigma_x^2\sum_{i=0}^\infty h^2(i)-2\sigma_x^2 \boldsymbol h^T \left[ \begin{matrix} \boldsymbol I_{N+1}\\\boldsymbol 0_\infty \end{matrix}\right]\boldsymbol w+\boldsymbol w^T \boldsymbol R_N\boldsymbol w
\end{split}
\tag{1.2}
\end{equation}
$$
如果计算导数也可以得到最优匹配为$$\boldsymbol w_0=\boldsymbol h_{N+1}$$，这个是$$N$$阶自适应滤波器的极限，其中后面这个$$\boldsymbol h_{N+1}$$是指$$\boldsymbol h$$的前$$N+1$$项，后面补零。

对于没有测量噪声或信道噪声的环境来说，如果未知系统冲激响应为有限长且自适应滤波器建模充分，那么最后的`MSE`可以为$$0$$。不过显然这个是不能避免的，因此结果总会有噪声的方差项。实际应用很多。

### 信号增强

滤波器常用的领域往往自适应滤波器也是常用的。信号增强的框图如下

<div style="text-align:center"><img alt="" src="https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/2.10.se.png" style="display: inline-block;" width="500"/>
</div>

在信号增强中，参考信号是受到加性噪声$$n_1(k)$$污染的信号，如图所示，而自适应滤波器的输入是另一个噪声信号$$n_2(k)$$。我们说这个噪声是与上一个噪声信号相关的，但是与$$x(k)$$无关。这种结构常被用在电源线干扰消除等应用中，在通信系统中消除杂波引起的回音也被认为是增强问题。此时误差信号就是
$$
\begin{equation}
\begin{split}
e(k)=& d(k)-y(k)\\=& x(k)+n_1(k)-\sum_{i=0}^N w_1n_2(k-l)=x(k)+n_1(k)-y(k)
\end{split}
\tag{1.3}
\end{equation}
$$
此时的`MSE`为
$$
\begin{equation}
\begin{split}
E[e^2(k)]=E[x^2(k)]+E\{ [n_1(k)-y(k)]^2 \}
\end{split}
\tag{1.4}
\end{equation}
$$
由于待增强信号与另一路噪声无关，因此认为这路噪声经过线性滤波后与待增强信号仍然无关。这样就可以轻易地消除掉交叉项。

如果我们将$$n_2$$作为输入信号，就可以通过某种方式预测到$$n_1$$，这取决于两者相关性。这样就能最大化输入信号，此时我们最小化`MSE`为
$$
\begin{equation}
\begin{split}
\xi_{min}=E[x^2(k)]
\end{split}
\tag{1.5}
\end{equation}
$$

### 信号预测

一般来说信号预测的滤波器结构为

<div style="text-align:center"><img alt="" src="https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/2.10.sp.png" style="display: inline-block;" width="500"/>
</div>

显然可以知道`MSE`
$$
\begin{equation}
\begin{split}
\xi=E\{ [x(k)-\boldsymbol w^T \boldsymbol x(k-L)]^2 \}
\end{split}
\tag{1.6}
\end{equation}
$$
最小化这个值可以得到一个`FIR`滤波器，可以通过过去的样本值预测当前样值。如果我们将`MSE`优化到足够小的水平，我们甚至可以认为这个`FIR`滤波器就是信号的模型。最小`MSE`的理论值为
$$
\begin{equation}
\begin{split}
\xi_{min}=r(0)-\boldsymbol w^T\left[\begin{matrix}r(L)\\r(L-+1)\\ \cdot\\\cdot \\r(L+N)\end{matrix}\right]
\end{split}
\tag{1.7}
\end{equation}
$$

### 信道均衡

上学期通信原理就学过信道均衡这个东西，当时用的就是横向滤波器，最简单的`FIR`结构。自适应信道均衡如下

<div style="text-align:center"><img alt="" src="https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/2.10.ce.png" style="display: inline-block;" width="500"/>
</div>

这个就是模拟了一个信道模型和一个直接延迟模型，设计延迟是因为`FIR`滤波器阶数导致的必要的延迟。

## LMS算法

这里的排版比较乱，很多东西现在还没有想好应该怎么叙述才比较好，所以很多东西只是直接贴出来，以便以后排版。

我们采用维纳滤波器是为了最小化均方误差，凡是最小化均方误差的滤波器实际上都是维纳滤波器。但是维纳滤波器在实践上有几个不能实现的地方，例如

- 维纳滤波器采用的是均方误差，但是实际应用中我们根本得不到统计平均值，只能通过测量得到当前值。
- 无法计算统计平均，只能利用当前值，就需要使用估计值替代。涉及的量有相关矩阵和我们定义的向量$$p$$

$$
\begin{equation}
\begin{split}
\hat {\overrightarrow R}(k)=\overrightarrow x(k)\overrightarrow x^T(k)\\
\hat {\overrightarrow p}(k)=\overrightarrow d(k)\overrightarrow x(k)
\end{split}
\tag{9}
\end{equation}
$$

以当前次的估计值代替实际值。而我们采用的最陡下降法，计算时

$$
\begin{equation}
\begin{split}
w(k+1)=w(k)-\mu \hat g_w(k)
\end{split}
\tag{10}
\end{equation}
$$

按照梯度的反方向进行更新，每次更新步长为$$\mu​$$。如果错误仍然定义为$$e=d-y\:​$$这样的方式，则错误平方的梯度估计值可以写作

$$
\begin{equation}
\begin{split}
\hat g_w(k)=-2e(k)x(k)
\end{split}
\tag{11}
\end{equation}
$$

注意这个估计值是通过直接带入$$g_w(k)=-2p(k)+2R(k)w(k)​$$，这个式子实际上就是我们在推导维纳解的时候通过对`MSE`求导得到的。

如果我们对这个估计值进行相同的求导我们会发现结果是一样的。实际上我们每次迭代时的均方误差与瞬时误差是**等价**的。*其实话不能这么说，因为等价关系并不显然，通过MSE计算的结果实际上是不包含k的，这里单独计算一次的，我们应该说形式上是完全相同的。*

那么把这个带进去就可以得到梯度更新公式
$$
\begin{equation}
\begin{split}
w(k+1)=w(k)+2\mu e(k)x(k)
\end{split}
\tag{12}
\end{equation}
$$
这个通过瞬时值而不是统计平均值方法得到的计算结果被称为`LMS`算法。这个平均的思想其实我并不知道在哪里。但是**最小**和**平方**都是很显然的。这种方法实际上只需要对反馈结果做一个相关延迟这样的处理，就可以实现自适应滤波。

### 梯度特性

前面介绍的，`MSE`方法搜索梯度方向为
$$
\begin{equation}
\begin{split}
g_w(k)=2[Rw(k)-p]
\end{split}
\tag{13}
\end{equation}
$$
而在`LMS`中，我们的梯度采用估计值
$$
\begin{equation}
\begin{split}
\hat g_w(k)=2[x(k)x^T(k)w(k)-d(k)x(k)]
\end{split}
\tag{14}
\end{equation}
$$
实际上这两个方向是很不相同的，但是从平均的意义上来看。对于固定抽头的滤波器（或者如果认为滤波器抽头系数与信号无关），此时对估计梯度值求统计平均就会得到`LMS`梯度方向的统计平均就是真正的梯度值。我们可以把这个梯度的估计当做是真实值的无偏估计值。

### 收敛特性

我们这里主要介绍要如何理解计算过程。我们假设测量过程中存在测量白噪声$$n(k)$$，那么在计算的时候假设误差信号与输入信号是独立的，就能在期望中去除一项，只留下$$w​$$项计算。

通过某种对角变换，我们能够将原来的相关矩阵变换为对角阵，通过一个统计平均的方式进行计算，可以得到某个形如$$I-2\mu R$$或者$$I-2\mu \Lambda$$的系数矩阵。这个系数往往有$$k$$次方，这时就要求这个系数矩阵的取值小于$$1$$，由此可以得到
$$
\begin{equation}
\begin{split}
\mu <\frac{1}{\lambda_{max}}
\end{split}
\tag{15}
\end{equation}
$$
这个取值可以保证结果是收敛的，不过是一个上界，实际取值只要小于这个上界就可以了。

### 系数误差向量协方差矩阵

这里实际上是推导二阶统计量。二阶统计量有啥意义我还不是很懂。我们先看一下系数误差向量的协方差

> 系数误差向量，实际上就是$$\Delta w(k)$$，这个定义为当前系数与维纳解系数之间的误差。如果我们观察这个量的协方差，就可以观测收敛过程的方差变化

$$
\begin{equation}
\begin{split}
con[\Delta w(k)]=E[\Delta w(k)\Delta w^T(k+1)]
\end{split}
\tag{16}
\end{equation}
$$

带入上面的计算结果，$$\Delta w(k+1)=[I-2\mu x(k)x^T(k)]\Delta w(k)+2\mu e_0(k)x(k)$$，考虑到最佳误差与输入信号之间的独立正交关系，可以去除相关项，最后可以得到，当$$k$$无限增大的时候，由于其中可见的激励项的存在，方差永远也不可能趋近$$0$$。

不过由于是最优化算法，人们一般喜欢将性质特殊的厄米特相关矩阵对角化进行计算，主要方法就是左右乘矩阵$$Q^T$$和$$Q​$$。后面有一段莫名其妙的推导，然后得出结论，稳定性条件
$$
\begin{equation}
\begin{split}
0<\mu<\frac{1}{2\lambda_{max}+\sum_{j=0}^{N}\lambda_i}<\frac{1}{\sum_{j=0}^N\lambda_i}<\frac{1}{tr[R]}
\end{split}
\tag{17}
\end{equation}
$$
实际应用中往往采用的是最后一个简化的表现形式。

这里会涉及一个较为复杂的推导过程，这里就不进行推导了，太复杂了懒得打上去。

### 误差信号的特性

如果测量噪声是加性噪声，我们就可以在实际测量结果后面直接叠加一个$$n(k)$$。这个结果实际上影响不大，因为后面的相乘只要涉及统计平均就会被消除。我们这里主要希望讨论的问题是，如果我们使用的是`FIR`自适应滤波器辨识一个`IIR`系统，会发生什么样的事情。实际上此处没加证明地认为，如果`FIR`建模不足，就会出现残留误差，即因为`FIR`点数不够多而只能辨识`IIR`系统的前几个点。

我们认为结果是这样的
$$
\begin{equation}
\begin{split}
E[e(k)]=E\left[ \sum_{i=N+1}^{\infty}h(i)x(k-i) \right]+E[n(k)]
\end{split}
\tag{18}
\end{equation}
$$
前一部分是目标信号中未被辨识的部分，如果输入信号的均值为$$0$$，那么我们就可以认为上述结果为$$0$$。即如果我们输入信号是一个均值为$$0$$的信号，那么我们认为`FIR`的估计是无偏的。（这么说对么）

### 最小均方误差（实际上是二阶统计量）

上一部分计算了误差信号的一阶统计量，即均值，此处要计算二阶统计量，均方误差`MSE`了。计算方式就是$$E[e^2(k)]$$

我们针对建模不充分（即上述的条件）与存在加性噪声的环境下。我们认为实际上参考信号可以表示为$$\boldsymbol h^T\boldsymbol x_\infty(k)$$。实际上这里的$$\boldsymbol h^T$$是一个无限长的函数，而$$\boldsymbol x_\infty$$也是一个无限长序列。我们利用矩阵巧妙将其分离为`FIR`可以识别和不能识别的两部分。计算`MSE`可得，最小均方误差为
$$
\begin{equation}
\begin{split}
\xi_{min}=&E[e^2(k)]_{min}=\sum_{i=N+1}^\infty h^2(i)E[x^2(k-i)]+E[n^2(k)]\\=&\sum_{i=N+1}^\infty h^2(i)\sigma_x^2+\sigma_n^2
\end{split}
\tag{19}
\end{equation}
$$

- 当自适应滤波器具有充分的阶数的时候，可以输出一个接近$$d(k)$$的信号，得到最小`MSE`
- 非充分建模的时候，我们会得到额外的`MSE`（如上第一项）以及（测量）噪声的方差

以上两小节就是对误差信号的一二阶统计量的分析结果。

### 超量均方误差与失调

这个翻译的很差劲，中文版的课本一点也看不懂，绝了。这个主要说的是啥问题呢，就是说梯度下降计算的时候，我们的结果可能收敛到维纳解，但也有可能收敛不到。如果收敛到了维纳解，我们就说这个误差是最优误差信号，如果没有，这个误差就会是最优误差信号与抽头偏差导致的误差之和，即
$$
\begin{equation}
\begin{split}
e(k)=e_0(k)-\Delta \boldsymbol w^T(k)\boldsymbol x(k)
\end{split}
\tag{20}
\end{equation}
$$
此时`MSE`的超量是可以计算的。这里计算的思想还需要亲自去体会一下，因为计算过程中涉及特征矩阵的部分全部采用对角变换将其变换成特征值对角阵，此时采用的对角变换阵是特征向量正交矩阵$$Q$$。这里继续计算推导可以得出
$$
\begin{equation}
\begin{split}
\Delta \xi(k)=\sum_{i=0}^{N}\lambda_i\nu_i'(k)
\end{split}
\tag{21}
\end{equation}
$$
这里给出的$$\nu$$其实是一个复杂表达式的简化。带入前面计算过的结果可以知道，超量误差为
$$
\begin{equation}
\begin{split}
\xi_{exc}=\lim_{k\to \infty}\Delta \xi(k)\approx \frac{\mu\sigma_n^2 tr[\boldsymbol R]}{1-\mu tr[\boldsymbol R]}\approx \mu\sigma_n^2 tr[\boldsymbol R]=\mu(N+1)\sigma_n^2\sigma_x^2
\end{split}
\tag{22}
\end{equation}
$$
如果是采用比值的形式，这个量就被称为是失调，结论为
$$
\begin{equation}
\begin{split}
M=\frac{\xi_{exc}}{\xi_{min}}\approx \frac{\mu tr[\boldsymbol R]}{1-\mu tr[\boldsymbol R]}
\end{split}
\tag{23}
\end{equation}
$$
可以看出，如果我们采用的$$\mu$$越小，则最后就会产生越小的失调（或超量误差）。但是这样也会导致收敛更慢。

### 瞬态特征

这里说的是，自适应滤波器的系数收敛是指数衰减的，可以用一个时间常数为$$\tau_{wi}$$的指数包络来近似。这一点课本描述不太好，老师决定现场展示一下。结论是，当我们认为依最慢收敛模式收敛100倍的时候就是完成收敛的时候，迭代次数约为
$$
\begin{equation}
\begin{split}
k\approx 4.6\frac{(N+3)\lambda_{max}}{2\lambda_{min}}\approx 2.3(N+3)
\end{split}
\tag{24}
\end{equation}
$$
多说一句，是这样的，由于特征值的不同，所以我们采用的系数$$\mu$$本应不同。但是由于计算的时候没有考虑这个参量，考虑的时候又有可能导致其他问题，因此一般的采用一个统一值进行计算。我们采用

`2019-3-29 10:49:55`

---

`2019-4-14 22:27:54`

半个月没更新了，其实现在啥也看不懂了。

## 基于LMS的算法

几乎都是以计算复杂度为代价，提升收敛速度。当然也有一个是降低计算复杂度，但是收敛时间是否降低另说。事实证明新的算法确实可以更快地收敛。

### 1. LMS-牛顿算法

牛顿法对信号的二阶统计量进行估计，避免当输入信号的相关性较强的时候收敛减慢。通过增加计算复杂度来提高收敛速度。

该算法首先计算先验误差，然后根据上一次估计的逆矩阵对本次迭代的逆矩阵进行估计。估计了本次的矩阵，就可以更新滤波器权重了。

牛顿法改进的一大创新点就是对相关矩阵**求逆**的递推方法。在逆矩阵的递推估计中，存在一个因子$$\alpha$$对过去信号与当前信号的取舍，一般是$$0\sim 0.1​$$之间取值。

---

老师为啥讲了一个**牛顿定理的实质**

牛顿方法实质就是`KLT`的最陡下降法。这么一说常见的算法实际上就只有**梯度下降**法呗？

我们首先考虑一个最陡下降算法，
$$
\boldsymbol w(k+1)=\boldsymbol w(k)-\mu \boldsymbol g_w(k)
$$
假如这里的收敛因子用$$\mu\boldsymbol R^{-1}$$代替，就得到了牛顿算法。

而什么是`KLT`呢，简单说就是特征值分解那种变换，将特征值对应的特征向量进行顺序排布、归一化处理，就得到了变换矩阵$$Q$$。这时候，新的参数均在变换域下，原来的相关矩阵就只有对角阵了。

正是由于变换域中的自相关阵为对角阵，因此我们认为输入信号已经被一组正交变换去除了相关性，去除了冗余信息。这就是这个变换的意义。我们得到的这个对角阵正好对应了每一个信号的功率。如果再借助一个归一化变换，我们就能将所有分量的功率归一化。归一化的滤波器系数与输入信号相乘与变换前的结果相同，即正交变换有内积不变性，自适应滤波器因此可以在变换域中进行。将变换域中的牛顿法进行反变换，得到的是最陡下降。

如果我们用`LMS`算法替代最陡下降，用常用的其他变换代替`KLT`，我们将会得到另一种常用的算法`TDAF`或`TDLMS`。

实际上`LMS`牛顿算法是企图对逆矩阵采用`KLT`估计。

---


### 2. 归一化LMS算法

`采用可变收敛因子使瞬时误差最小，可以缩短收敛时间但是增加了失调量`

同样为了提高收敛速度，改进是采用了可变的收敛因子$$\mu$$。改进的收敛因子可以随迭代次数进行更新，但是目的都是加速收敛。

例如我们尽可能减小瞬时平方误差$$e^2(k)$$，最小化方法就是求导并令导数为$$0​$$，这样就得到
$$
\mu_k=\frac{1}{2\boldsymbol x^T(k)\boldsymbol x(k)}
$$
这样就能将瞬时平方误差最小化，这样`LMS`更新方程也会一起变更。

一般而言在更新方程中为了控制失调量，常会选择采用一个固定的收敛因子，因为可变的收敛因子是根据瞬时平方误差得到的。为了防止$$x$$取值较小的时候出现较大的更新步长，因此采用一个很小的常数$$\gamma​$$附加在分母上，似乎有个拉普拉斯平滑与这个操作类似。

同样的，先计算先验误差，然后进行参数更新。为了保证算法稳定，固定的收敛因子一般在$$2$$以内，一般取值都小于$$1​$$

### 3. 变换域LMS算法

`降低特征值扩展，加速收敛`

仍然是对于相关性较强的信号。这种思路是，既然在当前条件下收敛较慢，我们就在一个变换域中进行运算，收敛与变换前是等价的，然后根据变换域的结果进行反变换。这个变换域的要求是，收敛要比变换前快。

算法的系统设计就是在信号输入前加一个变换器，输出结果反变换就能得到结果了。常用的变换是正交变换或说是酉变换。可以说其实相当于一种数据预处理算法。
$$
\boldsymbol s(k)=\boldsymbol T \boldsymbol x(k)
$$
举一个简单例子，就是将特殊位置椭圆经旋转和功率归一化得到圆。原本计算较为复杂的方程就会变得较为简单，收敛速度也会提升。

这时候相关矩阵可以描述为
$$
\boldsymbol R_s=\boldsymbol T \boldsymbol R \boldsymbol T^T
$$
对于满秩的矩阵而言，我们将得到对角化的矩阵，变换矩阵是特征值向量集合，这种标准正交变换被称为`KLT`。这种变换目前是最优变换。

`KL`变换实际上相当于，对信号进行特征向量提取后，采用归一化特征向量组成变换矩阵使其对角化的变换。实际上由于需要进行特征值分解，因此计算复杂度会高一些。常采用离散余弦变换、离散傅里叶变换等存在快速算法又接近`KLT`的酉变换进行变换。

### 4. 仿射投影算法

`数据重用算法，在相关性强的时候收敛速度加快`

这是一种数据复用的算法，可以重用前面的数据。这个算法增大了失调量，引入了可调收敛因子以求失调量与收敛速度的平衡。原本的算法是将一次信号输入，这个算法在使用过这一组数据之后，将该组数据保存下来，下一次还参与迭代，保存上限是`L`次数据。

显然这个时候，$$y,d,e$$等在此处都要写作向量的形式。

仿射投影算法的得名正是目标函数，使系数更新时相邻两次的误差最小化，并且增加约束条件使**后验误差**为$$0$$。采用的最小化代价函数为
$$
\frac 1 2 \mid\mid \boldsymbol w(k+1)-\boldsymbol w(k)\mid\mid^2
$$
这个方法被称为是最小距离原理。

采用拉格朗日乘数法将有约束条件的最优化问题转化为无约束最优化问题，代价函数就是
$$
F[\boldsymbol(k+1)]=\frac 1 2 \mid\mid \boldsymbol w(k+1)-\boldsymbol w(k)\mid\mid^2 +\lambda^T_{ap}(k)\left[d_{ap}(k)-X^T_{ap}(k)w(k+1)\right]
$$
计算该代价函数的梯度并将其置零可以推得拉格朗日乘数法算子的解，以及递推方程。参数更新方程中虽然没有出现后验误差，但是却有着后验误差最小化的思想。如果我们将更新后的参数定义为一个超平面，我们可以发现在这个超平面上后验误差均为$$0$$（毕竟这个超平面的定义就是这样的）。

更详细的说明就连中文我都看不懂了。。。不写出来丢人了。

### 5. 量化误差算法

利用短字长或`2`的幂次，降低计算复杂度。思想有点类似于傅里叶变换。






