---
layout: post
comments: true
title:  "Vanilla RNN"
excerpt: "-"
date:   2019-01-13 14:42:24 +0000
categories: Notes
---

<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
---

## Vanilla RNN

Vanilla原义是香草，此处含义是`without fancy stuff`，即最原初的版本。

本来想参考GitHub上的某个项目写，结果这个项目的主人本身也不懂什么是vanilla RNN，甚至于Gradient Decent的时候求导计算都是错的。下面还是通过简单计算推导来说明一下Vanilla RNN。

首先是RNN的最基础模型。RNN是Recurrent Neural Network，国内说法很多，有的地方翻译为循环神经网络，有的地方翻译为递归神经网络。这两个翻译个人认为均不准确。RNN是一种结构稍显特殊的网络，相比传统的深度网络和卷积网络这样的结构，RNN的结构并不是简单的层状结构，而是一种类似循环的结构，相比于普通的全连接网络或者CNN，RNN更适合于处理连续模型或时序模型。

其基础结构如下

![](https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/RNN1.png)

左侧是折叠后的网络结构，右侧是展开后的结构。这个结构是目前来讲最为基础的一种。如图，下方的$$x^t$$表示的是该时刻的输入，这个输入往往是一个向量，因此写成小写的向量形式；中间始终向右传递的$$h^t$$是`hidden state`，隐藏状态，这个值在不同的`RNN Cell`中不断传递并更新。每一个Cell的计算结果经过一个**激活函数**后将得到一个输出，这个输出与我们期望的输出进行比较，用`Loss function`来描述两者的**误差**。本文中可能将误差描述为损失或错误(**E**rror)。

图中的几个加粗大写字母是矩阵，但是使用这些矩阵难免造成混淆，因此此处对矩阵进行重命名。输入后的矩阵$$U$$命名为$$W_{hx}$$，状态间转化矩阵$$W$$命名为$$W_{hh}$$，输出矩阵$$V$$命名为$$W_{hy}$$。

该过程涉及如下计算

隐藏状态$$h$$更新

$$h^t=f(W_{hx}x^{t}+W_{hh}h^{t-1}+b_h)$$

输出计算

$$y^t=g(W_{hy}h^t+b_y)$$

以上计算与个人习惯有关，运算可以理解为，每一个`cell`输入前一时刻的状态$$h^{t-1}​$$，利用输入$$x^{t}​$$，通过一个非线性关系计算该时刻的状态$$h^t​$$，然后利用该时刻状态计算输出$$y^t​$$。此过程建立了输入与输出的联系，并且对其中循环更新的隐藏状态$$h​$$进行更新。

最简单的vanilla RNN应用是文字的预测，此处以字母为输入，依照输入指定长度的字符序列，预测接下来输出的字符序列。

字符的输入需要经过编码，否则不能直接参与运算。字符编码的方式很多，一般对编码的要求不同的字符有不同的编码即可。但是在较大的数据量下，编码方式会有更多要求。常见编码如下

>1. 独热码（one-hot-coding）。此编码会对字典内的所有字符进行一个编码，编码方式为，首先生成一个长度为字典大小$$m$$的零向量，然后将互不相同的一位编码为1。这样一来不但计算速度快，而且每个字符之间都是正交的。因此采用此类编码方式在本文所示的项目中十分合适。
>
>    独热码的另一个好处是有利于分类模型。这个在后面会有讨论。
>
>2. 词向量法（word2vec）。此编码方式通过一定的算法将单词向量化，最小处理单位是词。这种方法生成的词向量，在同义词之间有很强的相关性（表现为长度与方向的近似，以及相加时可以将词的表意相加而组成词组），而且是指定长度（维度）的，在字典超大的条件下可以降低内存使用。

这样，该模型输入$$x^t​$$就可以用向量的方式来表达。按照上图所示，假设模型输入是$$m​$$维的向量，例如

$$ x^t=\left[ \begin{matrix} x_0  \\ x_1  \\ x_2 \\ x_3 \\ ... \\ x_m  \end{matrix} \right] _{m \times 1}​$$ 



由于编码为独热码，因此可以写作

$$ x^t=\left[ \begin{matrix} 0  \\ ...  \\ 1 \\ ... \\  0  \end{matrix} \right] _{m \times 1}​$$



> 举例，假设字典中共有40个字符，则矩阵长度就是40，并将40个不同的字符对应不同位置为1的向量。假设第一个字符是A，则A编码如下

$$ x_A=\left[ \begin{matrix} 1  \\ 0  \\ ... \\  0  \end{matrix} \right] _{m \times 1}$$

另外假设隐藏状态$$h​$$的长度为$$n​$$，则计算过程可以写成矩阵

$$ h^t=\left[ \begin{matrix} h_1^t  \\ h_2^t   \\ ... \\ ... \\ h_n^t  \end{matrix} \right] _{n \times 1}=\mathfrak F\left( \left[ \begin{matrix} ... & W_{hx}(1,j) & ... \\... & ... & ...   \\ ... & W_{hx}(i,j) & ...  \\ ... & ... & ...  \\  ... & W_{hx}(n,j) & ...   \end{matrix} \right] _{n \times m} \left[ \begin{matrix} 0  \\ ...  \\ 1 \\ ... \\  0  \end{matrix} \right] _{m \times 1}+    \left[ \begin{matrix} ... & W_{hh}(1,j) & ... \\... & ... & ...   \\ ... & W_{hh}(i,j) & ...  \\ ... & ... & ...  \\  ... & W_{hh}(n,j) & ...   \end{matrix} \right] _{n \times n} \left[ \begin{matrix} h_1^{t-1}  \\ h_2^{t-1}   \\ ... \\ ... \\ h_n^{t-1}  \end{matrix} \right] _{n \times 1}        \right) $$ 

以上是对隐藏状态的更新

相似的，输出可以写作

$$ y^t=\left[ \begin{matrix} \hat y_1^t  \\ \hat y_2^t   \\ ... \\ ... \\ \hat y_m^t  \end{matrix} \right] _{m \times 1}=\mathfrak G\left( \left[ \begin{matrix} ... & W_{hy}(1,j) & ... \\... & ... & ...   \\ ... & W_{hy}(i,j) & ...  \\ ... & ... & ...  \\  ... & W_{hy}(m,j) & ...   \end{matrix} \right] _{m \times n} \left[ \begin{matrix} h_1^t  \\ h_2^t   \\ ... \\ ... \\ h_n^t  \end{matrix} \right] _{n \times 1}   \right) $$



通过上述计算，我们就完成了一个`cell`的计算。

上述操作将在不同的`cell`之间传播，整个网络相当于一个时序处理模型。现在我们需要调整系统，因为各个矩阵的权值都是初始化的时候自动生成的，因此并不能直接输出我们期待的结果。因此要设计算法，能够根据我们期待的输出结果对系统进行调整。最常见的能够动态调整的算法就是梯度下降法，这里会从梯度下降的角度介绍RNN的优化过程。

梯度下降的计算方法是，让目前的参数向错误减小的梯度方向进行一定步长的移动。首先我们需要规定一个能够衡量参数误差的函数，常命名为$$loss$$或$$E$$。简单常用的衡量方法有很多，取其中的两种。

>一、 **直接比较**，采用均方误差方法衡量，
>
>$$E=\frac{1}{2}(y-t)^2$$
>
>或
>
>$$MSE=\frac{1}{2m}\sum\limits_{i=1}^m(y-\hat y)^2​$$
>
>其中$$t$$是期待的正确输出，$$y$$是系统真实输出。采用这种方法，只需要设计算法让$$E$$向$$0$$逼近即可。二次方项在求导后会消失，计算起来也很方便。这个函数也是一个凸函数，因此在区间内的局部最优解就是全局最优解。

>二、 **交叉熵比较**，衡量两个概率分布的不同（距离）
>
>$$H(p,q)=-\sum\limits_i p(x_i)log\: q(x_i)$$
>
>或写作
>
>$$loss=-\sum\limits_{i=1}^n y_i\: log(\hat y_i)$$
>
>交叉熵的一般描述是这样的，在此处我们可以认为$$p\:\&\:q​$$是输出变量和实际变量的概率分布。至于为什么说是概率分布，因为输出层常常会做一个`Softmax`操作，这个操作可以将数值转化为一个具有概率意义的值，也可以说是一种归一化方法。
>
>得到概率分布后可以直接和独热码进行交叉熵比较，因为独热码也是一种概率分布，但这种概率分布是唯一的，在指定输出为$$1$$，其他位置都是$$0$$。因此在这种特殊条件下，损失函数就变成了
>
>$$loss = -log(\hat y_i)$$

不失普遍性，假设采用激活函数$$f(x)$$为`tanh`，采用输出层激活函数$$g(x)$$为`softmax`，损失函数为交叉熵。以下开始推导过程。









因为没有抽到阿比所以所以不是很想学习

他妈的心情好了再写

`2019-1-15 21:14:01`

fgo卖号了~

## Reference

[1] CS229 