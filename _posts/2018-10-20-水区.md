---
layout: post
comments: true
title:  "水区"
excerpt: "-"
date:   2018-10-20 12:42:24
categories: Notes
---

<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

没有意义的内容全部集中在这里了，不定期更新，日期后注。

---

### 1. ユメセカイ

[ユメセカイ - 戸松遥](http://music.163.com/song?id=417613546&userid=417359311)

第一部中桐老爷在医院醒来后的BGM。

> 想说一句，谷歌和百度的翻译很灵性。我不是很懂日语规则，但是谷歌翻译出来的画风是“Yumesekai”，而百度翻译出来就是“梦世界”。两家的NLP模型看来对语言的处理确实存在一定不同，对于片假名的解读，谷歌倾向于表达注音这个意思，而百度却指出了这个注音可能的含义。如果出现多义性问题，百度的表现就不得而知了w。
>
> 官方翻译应该是“梦想世界”，这段BGM出现的位置，原本标志着川原砾当年对“刀剑神域”这个世界观的描写的结束，但在后来这个故事还是继续延续下去了。从我个人角度来看，SAO的故事如果在这里结束，也是很好的了...后面对这个故事的不断增补，反而给人一种狗尾续貂的感觉。每当听到这个BGM，总会让我想到高三的时候，每天十点十分就会熄灯，而楼管大爷也会在十一点前不断地查房，检查宿舍纪律。楼道中的微弱灯光透过门上的玻璃照到我的床边，光斑大小刚好够放下一本“刀剑神域”。高三下学期最后的时候，很长一段时间，在十一点后我都会从枕头下面拿出这本书逐字逐句地仔细阅读，也算是我高中时期最辛苦的时段中唯一的乐趣了。虽然在看小说的时候从来没有看过动画作品，可是就算是第一次听到这个BGM我也自然地联想到了这个片段，可以说是非常用心的作品了。
>
> 从现实的角度来讲，我可能无论从哪个角度来看都是一个不合格的人。每当这样想的时候，都会想再来听几遍这样的歌曲，好让我尽快抛弃这样的想法。
>
> 2018年10月20日12:42:08

### 2. 微机原理课程设计

**这一段比较长**

TPU是特化的专用处理器。

- 通用处理器

  通用处理器一般指我们目前使用的CPU。相对于专用处理器，其性能没有针对某些特殊任务做特别的优化，因此在各项使用中表现都很正常，没有特别出色的地方。

  - **复杂指令集**（Complex Instruction Set Computing;**CISC**）通用处理器

    目前正在学习的x86（例如8086）就是基于复杂指令集的处理器。复杂指令集是一种微处理器指令集架构，每个指令执行若干低级操作（存储器中读取、存储，计算等，全部集于单一指令中）。复杂指令集指令数目多而杂，且每条指令字长并不相等，计算机必须加以判断读取，在性能上付出代价。

  - **精简指令集**（Reduced Instruction Set Computing;**RISC**）通用处理器

    对指令数目与寻址方式都做了精简，使其实现更容易，指令并行执行程度号，编译器的效率更高。目前而言采用精简指令集的处理器主要是ARM、AVR、MIPS以及IBM的Power Architecture处理器。

    > 题外话，早期计算机行业中编译器技术并不发达，许多程序以机器语言或汇编语言来完成。为了便于程序编写，当年的程序设计者设计出越来越复杂的指令，可以直接对应高级编程语言的高级功能。在那个年代的看法是，硬件设计更加容易一些。
    >
    > 当时的内存还极小，在缺乏内存的条件下，我们需要程序更加精简，毕竟每一字节都很宝贵。因此要对信息做高度编码。当时内存不仅小，而且还慢（磁芯时代），如果我们对信号做高度编码，访问频率可以下降。
    >
    > 上课也讲过寄存器的设计很贵，每多一位就要提高很多成本；而且以当时的设计水平，额外设计更大的寄存器也是难度较高的。
    >
    > 总体来说当时的人们就是又不愿意多花钱，又懒得自己动手，因此编个程序累skr人。微处理器设计师们也尽可能的让每一个指令做更多的工作，这就是复杂指令集。
    >
    > 精简指令集的优化思想主要是
    >
    > 1. 统一指令编码
    > 2. 泛用寄存器
    > 3. 单纯的寻址模式（复杂寻址模式被简单计算指令序列替代）
    > 4. 硬件支持少数数据模式（部分CISC计算机中有处理字节字符串的指令，这在RISC中一般是不会有的）

    明面上看，复杂指令集都是在服务一些性能相对较差的嵌入式设备处理器，大公司中似乎只有IBM在使用复杂指令集。以**Intel**为例，其不使用复杂指令集的原因是考虑到了代码的兼容性问题。实际上Intel在底层设计时也会采用复杂指令集，人家只是不放弃CISC。Intel在进行了多年的尝试（例如Itanium）后最终也发现，RISC和CISC的混合方法可能是最优的。

    > 参考资料1：[Quora: Why didn't Intel move from CISC architecture to RISC architecture?](https://www.quora.com/Why-didnt-Intel-move-from-CISC-architecture-to-RISC-architecture)
    >
    > 参考资料2：[Stack Overflow:Why does Intel hide internal RISC core in their processors?](https://stackoverflow.com/questions/5806589/why-does-intel-hide-internal-risc-core-in-their-processors)

- 专用处理器

  专用处理器常用于特殊任务的高效运算。相比于CPU在图像处理的运算能力上，GPU要强得多。这也是GPU被设计的原因。GPU在浮点运算中的性能远高于CPU，而且对并行计算适应性更高，在PC机中，这就是一块专用处理器。这里不再对专用处理器做展开说明，只对TPU做一点说明。

  21世纪是生命科学的世纪；近年间来看，21世纪似乎又是一个“人工智能”的世纪。人工智能的说法是不妥的，因为我们眼中的人工智能的突破进展，实际上是**处理器性能提升**与**数据量暴增**两个原因共同造成的。语音与图像数据量的增长，以及对海量数据的有效利用促成了目前的计算机视觉以及自然语言处理等行业的发展，其中也离不开深度学习方法的巨大贡献。这里就列举一个实际应用的例子来说明专用处理器的必要性及其性能。

  深度学习领域中的许多运算都是浮点运算，这样的运算让CPU计算起来速度就会慢很多。因此在对神经网络参数做训练时人们会采取GPU进行，其速度会达到CPU的若干倍。但是Google毕竟是Google，不仅在学术上要走在行业的尖端，就连设备也要。为了配合自己发布的深度学习框架TensorFlow，Google还特意设计了一款硬件设备用于深度学习上的运算加速，那就是TPU（**Tensor Processing  Unit**，参考资料[In-Datacenter Performance Analysis of a Tensor Processing Unit](https://arxiv.org/abs/1704.04760)，1704.04760）。文献中主要论证了TPU的可实现性以及其性能指标。

  神经网络中的计算主要是矩阵的乘法、加法与非线性函数。典型的一个运算过程是

  $$y=\sigma(Wx+b)$$

  其中，$$W$$和$$x$$都是矩阵。$$x$$代表运算过程中的变量，常被称为特征(feature)。$$W$$是权值，因此$$Wx$$就是简单的矩阵乘法计算。$$b$$被称为偏置(bias)，整个结构就是一个变量较多的线性函数。然后人们向这个线性运算中添加了一个非线性函数，使结果出现非线性量，$$\sigma$$函数就是一个非线性函数，nn中常见的有Sigmoid，tanh，ReLU，Softmax等。详细内容可以看CS229那篇博客（

  TPU所依赖的基本技术为

  1. 采用低精度（8bit）计算。8个二进制位也就是2个十六进制位。在深度学习的参数计算过程中，经常会对参数进行归一化处理，因为Scale过大或过小的参数可能会促进梯度爆炸与梯度消失，更会导致计算量暴增而降低处理速度。根据上面文献的信息，将数据位数缩减到8bit仍然能够满足运算需求，对深度学习的参数运算影响很小。另外还能显著降低功率（或者说提高 性能/功耗 的效率比值）。
  2. TPU中采用了特殊的运算结构（Systolic Array）。在国内统一翻译为脉动阵列，Systolic是心脏收缩的意思，由于并行数据流经硬件连线接入处理器节点网络，然后数据被组合处理合并或排序并导出成为计算结果，类似人的心脏收缩时的工作流程，因此得名。该阵列往往用于特定操作，例如大规模并行积分、**卷积**、相关或**矩阵**运算。
  3. TPU采用了更大的片上内存，以此来降低对计算机内存的访问
  4. 直接将神经网络中会用到的激活函数算法硬件实现，并为数据运算提供了高级指令。可以将来自TensorFlow的图式运算结构API调用直接转化乘TPU指令。
  5. 相比传统的计算芯片，TPU设计极为简单。

  2016年TPU首次发布，但在这之前就已经在Google的项目中服役，最为出名的项目包括Google街景服务以及DeepMind的围棋软件AlphaGo等。2017年谷歌I/O年会上发布了第二代TPU，2018年五月发布了第三代。初代的TPU在性能上并不能超越同期的GPU，但随着更新迭代，TPU在处理TensorFlow框架下运算的能力越来越强。就在不到两周前，Google发布了一个称为BERT的NLP模型，号称目前最强的NLP模型（确实强=-=）。该模型在训练时会使用到TPU。完全体BERT训练时使用到了64个TPU芯片，总共使用4天时间就完成了训练（这一次训练大约要消耗50000美元）；如果在NVIDIA的Tesla P100处理器上进行训练，恐怕需要8块P100芯片并行训练1年。

1. 详细介绍神经网络的运算

   神经网络的典型计算是

   $$y=\sigma(W·x+b)$$，这个是一个前向传播的过程，可以参考图片

   ![](https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/Andrew.png)

   每一个神经元上存储的是权重，输入特征x经过特殊的规则与每特定的权重进行乘法运算，可以等效为一个矩阵乘法过程——一个线性过程。增加了偏置后就相当于增加了一个加性值b。经过这样处理后的结果，对其进行$$\sigma$$函数操作，使其可以出现非线性特征。至此神经网络的一个前向传播计算完毕，计算的结果将进行下一层的运算。

2. TensorFlow是什么

   TensorFlow是谷歌开源的一款深度学习框架，实际上是C++底层框架的一个接口。该框架分为Python版和C版，使用起来十分方便，降低搭建深度学习系统的工程量。

3. 脉动阵列（Systolic Array）是如何实现的。Systolic Array实际上不属于SISD，SIMD，MISD，MIMD中的任意一个，而是一种特殊结构。[知乎链接](https://zhuanlan.zhihu.com/p/26522315)给了一个较为清晰的解释，我们可以理解为，将权值矩阵按规则排好顺序，将输入的数据也按顺序排好，然后将权值从上面送入，x从左侧送入，每一个cell进行一次乘法运算，就可以在几个时序后完成整个矩阵的运算。

   ![](https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/systolicarrayI.jpg)

   ![](https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/systolicarrayII.jpg)

   ![](https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/systolicarrayIII.jpg)

   ![](https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/systolicarrayIV.jpg)

   ![](https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/systolicarrayV.jpg)



2018年10月21日16:42:08

## 3. 刮墙

温暖的话语总是能够激励人心。

2018年10月22日10:21:20

## 4. 日常

GitHub是炸了吗，怎么push不上去的。

GitHub炸掉的这一段时间里面，发生了很多事情。我越来越感觉我接近人类就是一种错误。



1. 我不会接受任何我不认同的东西。
2. 我不擅长与人类相处，我很累，不喜欢社交。
3. 我坚决不会道歉，我做过的所有事情都会认为是正确的。
4. 纸片人和真实的人相比不过是一些数据罢了。但是相比真实的人，虚假的纸片人更能打动我。
5. 当我判定了任何人是不尊重我的时候我就会社交性拉黑，目前大学中已经拉黑两人。
6. 目前与我关系最好的无血缘关系的人是陌生网友和熟悉的纸片人。
7. 以上东西是我经历了二十年才发现的。

别tm主动找我，我不是好人。