---
layout: post
comments: true
title:  "Multiobjective Reinforcement Learning for Cognitive Satellite Communications Using Deep Neural Network Ensembles"
excerpt: "-"
date:   2019-02-08 14:42:24 +0000
categories: Notes
---

<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
---

## 备注

- 这个文章看起来英文水平比较好
- `2019-2-8 20:40:19`放你妈的狗屎这什么垃圾英语

## 摘要 - Abstract

未来的航天通信子系统将受益于人工智能相关算法控制的软件无线电。本文中我们提出了一种新的无线电资源分配算法，利用多目标强化学习与人工神经网络结合，管理可用资源和冲突任务为基础的目标。数千种可能的无线电参数组合的性能不确定性，以及无线电信道随时间的动态行为，产生连续的多维状态-动作空间，需要固定大小的存储的连续状态-动作映射，而不是传统的离散映射。此外，操作决策应与状态分离，以允许在线学习、性能监控和资源分配预测。所提出的方法利用了作者之前关于通过“虚拟环境探索”约束预测性能较差的决策研究。

仿真结果显示了不同通信任务中该方法的性能，为未来的研究参考提供了数值准确的基准。该方法也构成了核心认知引擎概念验证的一部分，交付给了NASA的John H. Glenn的研究中心在国家空间站上的SCaN Testbed无线电系统。

## I - 引入 - INTRODUCTION

> 你妈的废话又臭又长

2012年，由NASA John H. Glenn研究中心的**空间通信与导航**(`Space Communications and Navigation, SCaN`)小组领导的一个研究项目为**国际空间站**(`International Space Station, ISS`)提供了一个通信研究平台。该平台被称为`SCaN Testbed`，是由三个软件定义的无线电系统组成，旨在促进未来航空航天应用的在轨通信研究。天基通信系统的下一个前沿(`frontier`)是开发和测试**认知引擎**(`cognitive engines, CEs`)，利用在轨软件定义无线电(`Software-Deﬁned Radios, SDRs`)的潜力，用于未来的太空探索任务。

预计下一代天基通信系统将提供更高的灵活性，以便在具有挑战性的环境中更有效地运行，包括轨道动力学和大气和/或空间天气，或者当要求航天器在不可预测的条件下运行时。因此，需要**认知引擎**有效地分配资源以实现若干目标，每个目标具有一定的优先级，同时通信信道动态地改变。 **认知引擎**应考虑通信系统对其他航天器子系统的资源消耗的影响，同时分配多个不同的资源以实现多个目标。

认知无线电(`Cognitive Radio, CR`)拥有**认知引擎**，它可以利用跨不同网络层的环境感知，并且能够自主地执行**感知**、**学习**和**推理**(`perception, learning, and reasoning`)活动，以便根据当前节点的**硬件和软件能力**、**信道条件**和**操作需求**优化**资源分配**。

目前已经有一些简单地自适应技术部署到现实的应用中了，这些被作为未来完全认知系统的基石。例如，作为DVB-S2标准的一部分，**自适应编码调制**(`adaptive coding and modulation, ACM`)方案在卫星电视接收机的信号衰落事件期间调整无线电参数。另一个例子是**动态信道接入**的**频谱感应**(`Spectrum Sensing, SS`)，其中临时未使用的频谱被重新用于不同的应用。

过去已经使用认知无线电为案例研究提出了几种机器学习的技术。机器学习支持在线学习，这是认知引擎的核心功能。一些人已经具体研究过学习问题，例如基于机器学习的认知无线电问题与资源分配等。

这些自适应技术单独运行的时候效果很好，例如**自适应编码调制**有助于缓解衰落问题（这在[上一篇文章](https://psycholsc.github.io/notes/2019/01/22/Precoding-Scheduling-and-Link-Adaptation-in-Mobile-Interactive-Multibeam-Satellite-Systems.html)中已经介绍了），**频谱感应**允许次用户临时共享频段。基于机器学习的**认知无线电**算法还可以处理多目标问题等。然而这些方法通常只考虑不到五个可适应的无线电发射机参数和通信目标，并假设性能功能独立于通信信道，即操作环境。最流行的基于机器学习的算法之一的遗传算法，由于以批处理模式运行，因此对在线学习需求有一定限制。有人描述了分散频谱分配代理的分布式解决方案，其中强化学习被人工神经网络增强，该网络仅使用一个性能函数作为多用户相同资源的输入与输出值（对不起这里翻译不出来）。

因此据作者所知，上述技术都没有解决与考虑为天基通信系统提供多种无线电资源分配和通信目标，考虑到在线学习以及与上述环境动态相关的多种适应性参数和性能函数。有人试图解决空间应用中认知无线电的学习问题，提出了一种基于强化学习的方案，混合人工神经网络的混合解决方案等。然而这些解决方案受限于以下假设

- 无衰落信道
- 离散固定大小的状态与动作空间
- 状态-动作状态是可以存储的

这些假设导致基于机器学习的解决方案的实际应用十分有限。根据载波频率、航天器轨道动力学、大气条件、空间气象条件和机载可用存储器，多目标资源分配是一个巨大的挑战，需要一种新的解决方案，以便在不可预测的条件下进行操作。

本文中提出了一个新的天基通信系统的**认知引擎**的设计，解决了上述的局限性。该认知引擎在试图`在动态变化的通信信道中实现多个冲突目标`时会自动选择多个无线电发射机设置。利用了前人介绍的强化学习结构，提出了一个称为`virtual exploration`的升级版本，并将其与一种新的深度神经网络的集成设计 和两种新算法结合而成，以实现**多目标强化学习**(`multi-objective reinforcement learning, MORL`)的开发部分（？）。由此，该认知引擎能够实现下述的所有目标

- 具有固定内存大小的无表的 状态-动作 映射（内存占用固定，相对前面的内存足够大）
- 在动态变化的信道上运行
- 状态与决策分离
- 使用连续的动作与状态空间

这些特点是以`处理需求来训练神经网络并用它们来预测`为代价实现的。总之神经网络的引入消除了强化学习中的状态-动作表和Q值，也是因此将状态与动作解耦，允许将一个动作映射到几个其他状态，从而实现对动态信道的操作。最终连续的空间导致了接近最优解决方案（这又是说了句什么玩意）。

第二部分简述了本项目中使用的机器学习概念，第三部分描述了解决方案，第四部分仿真，第五部分得出结论。

## II - 机器学习概述 - MACHINE LEARNING OVERVIEW

机器学习是一个用于描述自动执行计算决策任务的若干理论和算法的术语。与本文相关的，机器学习值得注意的两个研究进展为

- 2015年深度Q网络(`Deep Q-Network, DQN`)用于`Atari`游戏时有比人类的更优表现。
- 2016年`AlphaGo`赢得了围棋世界冠军

以上的两个决策系统基于的原理一直在推动机器学习在更多不同领域的研究。目前而言主要的驱动因素是计算机视觉系统，其应用主要在自动驾驶汽车等。

当然这些系统都是利用了本文中介绍的深度神经网络和强化学习方法的。最近的技术革命激发了使用以上控制概念进行卫星通信的提议，但仅管如此，应用时许多要求是完全不同的。据作者所知，这些要求导致了目前文献中没有的算法的研究和开发（废你妈话）。利用强化学习方法和深度神经网络基本原理，本文提出了混合方法设计，并对结果进行了仿真与讨论。

### A - 神经网络概述 - Neural Networks Overview

人工神经网络是一种将输入映射到输出（译者注，通常是非线性映射，因为采用了激活函数）的方法，通常用于模式识别(`pattern recognition`)问题或函数拟合(`function-ﬁtting`)问题。由三层甚至更多层神经元组成的网络通常被称为深度网络。在本文中，深度网络通过将**行动**(`action`)映射到**奖励**(`reward`)、将**状态**(`state`)映射到**行动**(`action`)，近似非线性的环境影响。

神经网络算法基本上由两部分步骤构成，即训练和预测。这是监督学习方法，首先将输入与输出对应的训练集输入进行训练，在损失函数下满足了一定的性能要求后，该网络就可以用来进行预测了。这里对该算法不做详细介绍。

目前的文献资料中没有关于如何选择神经网络的结构的说明，因此本文中选择了简单的全连接网络，其设计细节在后面会介绍。

### B - 强化学习概述 - Reinforcement Learning Overview

强化学习算法通过与环境交互反复试错的方式进行学习。基于预定目标，强化学习**智能体**`agent`会查找实现这些目标时优化系统性能的决策(`action`)。在传统的强化学习算法中，`agent`根据离散时刻$$k$$的`exploration probability function`$$f(\varepsilon)$$计算的`exploration probability value`$$\varepsilon_k$$在`exploitation`与`action`之间进行交替选择。

> 即智能体根据一定概率进行其他决策的探索，否则就直接决策而不探索其他决策方式。

强化学习问题可以被建模为**状态转移问题**，而状态转移问题本身就可以被建模为马尔科夫决策过程(`Markov Decision Process, MDP`)。本文中，状态转移假定是确定性的，`action`是应用一组无线电发射机参数，`state`是一组相关的通信系统性能值。有关`action and state`的更多详细说明，请参阅后文。

通常控制问题需要计算将观察到的`state`映射到`action`的`policy`。本文介绍的方法设计控制无线电参数，以便在信道条件发生变化时，性能在整个时间内保持为最佳水平。此时的`environment`由卫星通信信道组成，主要影响因素为发射机与接收机的视线路径、其附近的环境（地面站附近的建筑物或航天器附近天线的结构）以及大气与空间天气动态。如果采用状态转换和`action-state`模型，就必须考虑这些高度复杂的动态过程中的所有变量。出于这个原因，假设这些模型由于过于复杂或难以获取(`obtain`)而被认为是（效果）未知的，难以达成的，并且考虑到冲突目标，期望平衡探索新行动和利用已知行为的学习方法建议`action`。

应该通过下式给出的贪婪策略`policy`，对状态s下可能的所有行动，评估 表示在遵循`policy`$$\pi$$时处于`state`$$s$$时所采取的特定`action`$$a$$的值的`action-value`函数$$Q_\pi(s,a)$$

$$\pi(s)=arg\max\limits_a(Q(s,a))\tag{1}$$







## III - 提出解决方案 - PROPOSED SOLUTION



## IV - 结果 - RESULTS



## V - 结论 - CONCLUSIONS

