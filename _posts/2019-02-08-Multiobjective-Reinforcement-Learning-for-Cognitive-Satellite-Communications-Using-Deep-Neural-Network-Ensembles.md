---
layout: post
comments: true
title:  "Multiobjective Reinforcement Learning for Cognitive Satellite Communications Using Deep Neural Network Ensembles"
excerpt: "-"
date:   2019-02-08 14:42:24 +0000
categories: Notes
---

<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
---

## 备注

- 这个文章看起来英文水平比较好
- `2019-2-8 20:40:19`放你妈的狗屎这什么垃圾英语
- 用了巨多从句，整的谷歌都翻译不出来 = =、
- 这篇论文话好多
- 我发现每天的工作就是打开`Acrobat`，打开`Tpyora`，打开谷歌翻译和`Github`，然后开始复制粘贴和瞎BB
- 可以说本文作者很喜欢故弄玄虚了。
- 神经网络被他们当做了无敌的非线性拟合器，ε=(´ο｀*)))

## 摘要 - Abstract

未来的航天通信子系统将受益于人工智能相关算法控制的软件无线电。本文中我们提出了一种新的无线电资源分配算法，利用多目标强化学习与人工神经网络结合，管理可用资源和冲突任务为基础的目标。数千种可能的无线电参数组合的性能不确定性，以及无线电信道随时间的动态行为，产生连续的多维状态-动作空间，需要固定大小的存储的连续状态-动作映射，而不是传统的离散映射。此外，操作决策应与状态分离，以允许在线学习、性能监控和资源分配预测。所提出的方法利用了作者之前关于通过“虚拟环境探索”约束预测性能较差的决策研究。

仿真结果显示了不同通信任务中该方法的性能，为未来的研究参考提供了数值准确的基准。该方法也构成了核心认知引擎概念验证的一部分，交付给了NASA的John H. Glenn的研究中心在国家空间站上的SCaN Testbed无线电系统。

## I - 引入 - INTRODUCTION

> 你妈的废话又臭又长

2012年，由NASA John H. Glenn研究中心的**空间通信与导航**(`Space Communications and Navigation, SCaN`)小组领导的一个研究项目为**国际空间站**(`International Space Station, ISS`)提供了一个通信研究平台。该平台被称为`SCaN Testbed`，是由三个软件定义的无线电系统组成，旨在促进未来航空航天应用的在轨通信研究。天基通信系统的下一个前沿(`frontier`)是开发和测试**认知引擎**(`cognitive engines, CEs`)，利用在轨软件定义无线电(`Software-Deﬁned Radios, SDRs`)的潜力，用于未来的太空探索任务。

预计下一代天基通信系统将提供更高的灵活性，以便在具有挑战性的环境中更有效地运行，包括轨道动力学和大气和/或空间天气，或者当要求航天器在不可预测的条件下运行时。因此，需要**认知引擎**有效地分配资源以实现若干目标，每个目标具有一定的优先级，同时通信信道动态地改变。 **认知引擎**应考虑通信系统对其他航天器子系统的资源消耗的影响，同时分配多个不同的资源以实现多个目标。

认知无线电(`Cognitive Radio, CR`)拥有**认知引擎**，它可以利用跨不同网络层的环境感知，并且能够自主地执行**感知**、**学习**和**推理**(`perception, learning, and reasoning`)活动，以便根据当前节点的**硬件和软件能力**、**信道条件**和**操作需求**优化**资源分配**。

目前已经有一些简单地自适应技术部署到现实的应用中了，这些被作为未来完全认知系统的基石。例如，作为DVB-S2标准的一部分，**自适应编码调制**(`adaptive coding and modulation, ACM`)方案在卫星电视接收机的信号衰落事件期间调整无线电参数。另一个例子是**动态信道接入**的**频谱感应**(`Spectrum Sensing, SS`)，其中临时未使用的频谱被重新用于不同的应用。

过去已经使用认知无线电为案例研究提出了几种机器学习的技术。机器学习支持在线学习，这是认知引擎的核心功能。一些人已经具体研究过学习问题，例如基于机器学习的认知无线电问题与资源分配等。

这些自适应技术单独运行的时候效果很好，例如**自适应编码调制**有助于缓解衰落问题（这在[上一篇文章](https://psycholsc.github.io/notes/2019/01/22/Precoding-Scheduling-and-Link-Adaptation-in-Mobile-Interactive-Multibeam-Satellite-Systems.html)中已经介绍了），**频谱感应**允许次用户临时共享频段。基于机器学习的**认知无线电**算法还可以处理多目标问题等。然而这些方法通常只考虑不到五个可适应的无线电发射机参数和通信目标，并假设性能功能独立于通信信道，即操作环境。最流行的基于机器学习的算法之一的遗传算法，由于以批处理模式运行，因此对在线学习需求有一定限制。有人描述了分散频谱分配代理的分布式解决方案，其中强化学习被人工神经网络增强，该网络仅使用一个性能函数作为多用户相同资源的输入与输出值（对不起这里翻译不出来）。

因此据作者所知，上述技术都没有解决与考虑为天基通信系统提供多种无线电资源分配和通信目标，考虑到在线学习以及与上述环境动态相关的多种适应性参数和性能函数。有人试图解决空间应用中认知无线电的学习问题，提出了一种基于强化学习的方案，混合人工神经网络的混合解决方案等。然而这些解决方案受限于以下假设

- 无衰落信道
- 离散固定大小的状态与动作空间
- 状态-动作状态是可以存储的

这些假设导致基于机器学习的解决方案的实际应用十分有限。根据载波频率、航天器轨道动力学、大气条件、空间气象条件和机载可用存储器，多目标资源分配是一个巨大的挑战，需要一种新的解决方案，以便在不可预测的条件下进行操作。

本文中提出了一个新的天基通信系统的**认知引擎**的设计，解决了上述的局限性。该认知引擎在试图`在动态变化的通信信道中实现多个冲突目标`时会自动选择多个无线电发射机设置。利用了前人介绍的强化学习结构，提出了一个称为`virtual exploration`的升级版本，并将其与一种新的深度神经网络的集成设计 和两种新算法结合而成，以实现**多目标强化学习**(`multi-objective reinforcement learning, MORL`)的开发部分（？）。由此，该认知引擎能够实现下述的所有目标

- 具有固定内存大小的无表的 状态-动作 映射（内存占用固定，相对前面的内存足够大）
- 在动态变化的信道上运行
- 状态与决策分离
- 使用连续的动作与状态空间

这些特点是以`处理需求来训练神经网络并用它们来预测`为代价实现的。总之神经网络的引入消除了强化学习中的状态-动作表和Q值，也是因此将状态与动作解耦，允许将一个动作映射到几个其他状态，从而实现对动态信道的操作。最终连续的空间导致了接近最优解决方案（这又是说了句什么玩意）。

第二部分简述了本项目中使用的机器学习概念，第三部分描述了解决方案，第四部分仿真，第五部分得出结论。

## II - 机器学习概述 - MACHINE LEARNING OVERVIEW

机器学习是一个用于描述自动执行计算决策任务的若干理论和算法的术语。与本文相关的，机器学习值得注意的两个研究进展为

- 2015年深度Q网络(`Deep Q-Network, DQN`)用于`Atari`游戏时有比人类的更优表现。
- 2016年`AlphaGo`赢得了围棋世界冠军

以上的两个决策系统基于的原理一直在推动机器学习在更多不同领域的研究。目前而言主要的驱动因素是计算机视觉系统，其应用主要在自动驾驶汽车等。

当然这些系统都是利用了本文中介绍的深度神经网络和强化学习方法的。最近的技术革命激发了使用以上控制概念进行卫星通信的提议，但仅管如此，应用时许多要求是完全不同的。据作者所知，这些要求导致了目前文献中没有的算法的研究和开发（废你妈话）。利用强化学习方法和深度神经网络基本原理，本文提出了混合方法设计，并对结果进行了仿真与讨论。

### A - 神经网络概述 - Neural Networks Overview

人工神经网络是一种将输入映射到输出（译者注，通常是非线性映射，因为采用了激活函数）的方法，通常用于模式识别(`pattern recognition`)问题或函数拟合(`function-ﬁtting`)问题。由三层甚至更多层神经元组成的网络通常被称为深度网络。在本文中，深度网络通过将**行动**(`action`)映射到**奖励**(`reward`)、将**状态**(`state`)映射到**行动**(`action`)，近似非线性的环境影响。

神经网络算法基本上由两部分步骤构成，即训练和预测。这是监督学习方法，首先将输入与输出对应的训练集输入进行训练，在损失函数下满足了一定的性能要求后，该网络就可以用来进行预测了。这里对该算法不做详细介绍。

目前的文献资料中没有关于如何选择神经网络的结构的说明，因此本文中选择了简单的全连接网络，其设计细节在后面会介绍。

### B - 强化学习概述 - Reinforcement Learning Overview

强化学习算法通过与环境交互反复试错的方式进行学习。基于预定目标，强化学习**智能体**`agent`会查找实现这些目标时优化系统性能的决策(`action`)。在传统的强化学习算法中，`agent`根据离散时刻$$k​$$的`exploration probability function`$$f(\varepsilon)​$$计算的`exploration probability value`$$\varepsilon_k​$$在`exploitation`与`action`之间进行交替选择。

> 即智能体根据一定概率进行其他决策的探索，否则就直接决策而不探索其他决策方式。

强化学习问题可以被建模为**状态转移问题**，而状态转移问题本身就可以被建模为马尔科夫决策过程(`Markov Decision Process, MDP`)。本文中，状态转移假定是确定性的，`action`是应用一组无线电发射机参数，`state`是一组相关的通信系统性能值。有关`action and state`的更多详细说明，请参阅后文。

通常控制问题需要计算将观察到的`state`映射到`action`的`policy`。本文介绍的方法设计控制无线电参数，以便在信道条件发生变化时，性能在整个时间内保持为最佳水平。此时的`environment`由卫星通信信道组成，主要影响因素为发射机与接收机的视线路径、其附近的环境（地面站附近的建筑物或航天器附近天线的结构）以及大气与空间天气动态。如果采用状态转换和`action-state`模型，就必须考虑这些高度复杂的动态过程中的所有变量。出于这个原因，假设这些模型由于过于复杂或难以获取(`obtain`)而被认为是（效果）未知的，难以达成的，并且考虑到冲突目标，期望平衡探索新行动和利用已知行为的学习方法建议`action`。

应该通过下式给出的贪婪策略`policy`，对状态s下可能的所有行动，评估 表示在遵循`policy`$$\pi$$时处于`state`$$s$$时所采取的特定`action`$$a$$的值的`action-value`函数$$Q_\pi(s,a)$$

$$\pi(s)=arg\max\limits_a(Q(s,a))\tag{1}​$$

对于每一个$$s\in S$$，`policy`都会选出一个$$Q(s,a)$$最大的$$a\in A$$。

在包含数千个`action`$$a$$的连续或离散的$$A$$的问题时，在`state`$$s$$评估所有$$Q(s,a)$$可能是不行的。这里是无线通信的情况，在无线通信中，探索每一个`action`$$a$$可能会花费很多时间，并且迫使通信系统经历较为严重的性能下跌。实际的替代方案是让`agent`保持探索的动作。

一些文献中提到，认知无线电对即时`reward`敏感，并且任何`action`可以从任意`state`采取，而不需要计划在线申请。这些假设导致`Bellman Q-value function`稍加修改

$$Q_{k+1}(s_k,a_k)=Q_k(s_k,a_k)+\alpha \left[ r_k-Q_k(s_k,a_k) \right]\tag{2}$$

$$Q_{k+1}$$是得到`reward`$$r_k$$之后更新的`Q-val`，根据参数可以知道，此处的条件仍然是`state`$$s_k$$与`action`$$a_k$$，$$\alpha$$是学习率。即使状态转移与`state-action`模型是未知的，$$f(\varepsilon)$$和`reward function`$$\rho$$，其应用如下，仍然需要被定义。

$$r_k=\rho(s_k,a_k)\tag{3}$$

`state-action`映射函数

$$a_k=h(s_k)\tag{4}$$

在算式$$(1)​$$中给出了。

### C - RLNN 概述 - RLNN Overview

已经有先人提出过**多目标强化学习**(`MORL`)算法。也已经有人率先将前人的结果融合创新，设计了`NN-based RL, RLNN`，例如有将两篇文献中强化学习与神经网络结合在一起的人，假定时不变`AWGN`同步卫星通信信道进行设计的。它使用神经网络来虚拟地测试不同`action`，并测量它们的性能，从而允许对环境进行“虚拟探索”。

训练后，`RLNN`预测所有`actions`的多目标效能分数（即不同目标的加权和）。接下来根据性能阈值来将`action`分类为好或坏，即无线电参数的设定是否合适，并将所有效能得分计算为最高效能的百分比。然后从任意一类中依`action rejection probability`选取一个`action`。图1说明了被RLNN探索算法拒绝的`action`的时间序列性能的一个示例，这些`action`被预测在低于0.7的阈值下执行。相反，它建议执行高于该阈值的动作。

<div style="width:50%; margin-left:auto; margin-right:auto; margin-bottom:8px; margin-top:8px;">
<img src="https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/RLNNFig1.png" alt="" >
</div>
> 一段50秒时间序列示例，图中是RLNN仅采用虚拟探索，并将拒绝概率设置为1。在前200个数据包时，神经网络收集训练数据。显然，我们设置的阈值0.7，则预测出来性能不高于0.7的`action`的**探索**都被拒绝了。避免了不利的探索。

强化学习神经网络的一个重要好处在于知道每个动作的预期性能值（先验）,从而允许它们被分类。另一个好处就是可以通过`action rejection probability`来控制探索好的或坏的`action`的数量，通过滤除坏的`action`来提高系统整体的性能，`agent`可以避免花费时间去探索那些预测出来性能表现不佳的`action`。

上述的算法的主要缺陷是没有考虑到动态信道的问题。下图展示了一个时序示例，其中信道被假设为静态信道，给出的是每个动作的多目标性能得分。根据轨道的动态变化，载波频率、大气或空间天气可能会导致信道出现快速或慢速衰落，从而使其产生动态变化。

<div style="width:60%; margin-left:auto; margin-right:auto; margin-bottom:8px; margin-top:8px;">
<img src="https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/RLNNFig2.png" alt="" >
</div>


> 强化学习算法在同步卫星通信信道上搜索可用的更好`action`，搜索前首先考虑上一次探索到的`action`性能表现值。从图中可以明显看出决策`action`在不同时间段内的最优决策方案，连续位置可以覆盖整个时间段，此处的`action`就是系统的最优决策。

使用过的每一个离散的`action`都将其得分值存储在`Q-vector`中（假设`reward`与`state`是相同的），这些值在探索期间接收新值并更新现有值。

但是如果信道是动态变化的，这种方法就有三种主要缺陷

- 现在采取某种`action`的性能值得分可能和过去或将来采取同一个`action`的不同（这不是废话么，`action`的得分本来就是受时间和环境影响的啊，硬加一个缺陷还行）

  - 之前计算的性能值在后来使用的时候就过时了，这将导致采用某种`action`的时候做出错误决策。此问题的解决方法就是更新对应的性能值。这对于非动态变化的信道而言似乎没有大问题，但是对于动态变化的信道，某个特定的`action`可能在不同的环境下有不同的性能表现，之前探索计算的性能得分就无效了。下图很好的说明了这一点。该图片描述了两个不同的`action`随信道变化的仿真性能。即使是对于不同的通信任务，有的`action`也会表现出微小的变化，但是有的`action`随时间推移而产生的性能变化是剧烈非线性的。下方左右两图的剧烈反差实际上推动了本次研究。

- 必须为每个不同的多维`state`储存特定`action`的性能表现值，其中`state`是由连续变量表示的

  - 该缺陷导致需要大量的存储器（指数增加的存储器），这是由于当假设信道条件离散改变时，每个动作表现为不同性能水平。实际上状态是连续变化的，原计算需要更改如下

    $$Q_{k+1}(s(t),a_k)=Q_{k}(s(t),a_k)+\alpha[r_k(t)-Q_{k}(s(t),a_k)]\tag{5}$$

    这就表示给定`action`的得分随时间的动态行为。

- 必须为动态变化的特定信道条件存储每个`action-state`的性能表现值，但是信道变化也是连续的。

  - 该缺陷 增加了性能值的另一个维度，即随时间的变化，让数据的存储变的更为困难不可实现。从在线系统操作的实现角度来看，保存所有信息是不切实际而且不可能的。

<div style="width:100%; margin-left:auto; margin-right:auto; margin-bottom:8px; margin-top:8px;">
<img src="https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/RLNNFig3.png" alt="" >
</div>

> 两个不同动作$$a_A$$和$$a_B$$的多目标性能分别如图所示。在环境变化的整个过程中，保持`action`恒定，遵循的`SNR`配置也在图中显示。$$a_A$$在不同任务的表现随时间的变化并不明显，但是$$a_B$$的表现显然发生了明显的突变。

`2019-2-9 22:31:29`

本段还有很多内容没有完全看明白，还需要打磨一下。


## III - 提出解决方案 - PROPOSED SOLUTION

在继续提出算法之前，需要提到两个术语的区别，即强化学习的`state`和环境的`state level`。强化学习的`state`是`agent`进行观测计算的系统性能表现水平，它可以响应（反映？）强化学习中某个`action`的实施和现在的环境状态。在下一部分会说明，强化学习`state`是一个通信系统`fitness function`的一个特征。

> 之前把所有的fitness score 都翻译成了得分，主要是词穷，中文不好。后面基本上就不翻译了，有空把前面的也改了。

这里仿真的卫星通信中，我们假设信道发生的所有变化都能被传达（~~都能被届到~~），使用接收机处的`SNR`作为评价标准。也正是这样，`environment state`也被称为信道条件，信道状态等。

<div style="width:75%; margin-left:auto; margin-right:auto; margin-bottom:8px; margin-top:8px;">
<img src="https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/RLNNFig4.png" alt="" >
</div>

> 用于强化学习的探索(`exploration`)过程的深度网络，收到相同的多维输入进行输入。这里需要并行训练若干神经网络，并将其所有的预测输出平均为单个多目标性能值(`multi-objective performance value.`)。

对于动态变化的环境，强化学习神经网络现在要考虑到输入的$$\frac{E_s}{N_0}$$，这一部分是`Exploration NN`（如图）。该部分实际上是由相同训练数据集训练而成的几个深度神经网络`DNN`的集合，用于在相同输入的条件下生成多目标性能值。这些输出经过求平均后生成最终的预测值，也是如图所示。关于用于探索的网络结构，是采用`LM Algorithm`进行训练的前馈网络。这个`LM Algorithm`我有空写一篇文章出来说明。具体结构是三层全连接神经网络，只有`weight`而没有`bias`。两个隐藏层各包含`7`个和`50`个神经元，因此有$$449$$个参数（这里的想法是，输入7个神经元，然后过一个7的隐藏层，一个50的隐藏层，最后一层是一个输出神经元，$$7\times 7+7\times 50+50\times 1=449$$，这里感谢[@GeneZC](https://github.com/GeneZC)提供的帮助，哭了我这个不开窍的脑子），每层都采用`log-sigmoid`传递函数进行激活，输出层只有一个神经元，采用标准的线性映射函数。对于误差函数，此处采用经典的均方误差(`mean-square error`)作为性能的误差指标，其停止训练的条件有两个，

- 最小误差的梯度达到$$10^{-12}$$，这个很苛刻了，一般来说这么小的误差很有可能发生过拟合了。
- 最大验证校验达到$$20$$，这个也蛮苛刻的，`MATLAB`给出的默认值是6，即`validation set`连续迭代6次后误差不再下降，训练终止。这里设置为20，即迭代20次后不下降。过大的值一样容易导致过拟合。

训练的时候，数据集被**随机**分为$$7:3$$两部分，常见做法。$$70\%$$将用于进行训练(`training set`)，剩余的$$30\%$$还要对半分，平均分为两部分，$$15\%$$的部分用于测试(`testing set`)，另外的$$15\%$$的部分用于验证(`validation set`)。对应部分的用法在《统计学习方法》这本书里面有详细的说明，另外训练时的数据全部都按比例缩放到$$[-1,1]​$$范围内（这个是`scaling`操作，在[很久前的文章](https://psycholsc.github.io/notes/2018/08/10/CS229-Machine-Learning.html)里有介绍）。这个网络集合中有20个相同的网络结构，均采用相同的数据集进行训练，其输出结果用来对不同的`action`进行分类，分类依据是`action select agent`提供的性能表现阈值。另有`action rejection probability`控制从好的或坏的集合中随机选取动作，该动作即无线电参数的选择。

我们自己提出的混合`MORL Algorithm`（文中自称为先人`RLNN`的改良版`RLNN2`）采用两种不同的神经网络，一组用于`exploration`，另一组用于`exploitation`，其结构如下。

<div style="width:75%; margin-left:auto; margin-right:auto; margin-bottom:8px; margin-top:8px;">
<img src="https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/RLNNFig5.png" alt="" >
</div>

> 新型的为动态卫星通讯信道设计的`MORL`算法结构。此处我们提出的`RLNN2`和同样我们提出的探索与开发 神经网络 相互作用。

该结构将前面的神经网络和我们自己的新算法(`Exploitation NN`)进行了结合。这种新的算法可以处理卫星通信信道内的动态变化的衰减水平。该算法(`RLNN2`)的`agent`与环境进行交互，或者(`either`)通过虚拟探索`virtual exploration`探索不同的`action`（这将防止通信系统花费额外的时间探索可能性能不佳的参数组合），又或者(`or`)直接尝试曾经已经试过的`action`，为当前变化的信道预测最优的`action`（作者这里起了一个很中二很傻逼的名字，多维动作预测器，`multi-dimensional action predictor`）。

`Exploiting Reinforced Multi-Dimensional Actions:`为了解决前面第II部分提出的限制，我们删掉了`Q-value`计算的过程，而是到当前的环境状态水平（就是通信信道条件），建议使用另一个神经网络来预测应该使用哪个`action`。提出的这个神经网络的特征应该包括

- 解决使用过时的性能指标来决定使用哪个`action`的问题
- 不需要存储所有环境条件下所有动作状态的性能值
- 允许强化学习`state`与`action`相互分离（解耦、不相关等），这将允许在环境动态变化时通过开发不同的`action`来达到与之前相同的性能水平。

因此，我们提出的`NN2`结构是由一组神经网络阵列构成的，阵列中的每一个网络用于预测多维`action`中的一个维度。所有的`exploitation NN`具有相同的浅层结构，并接受相同的多维性能指标作为输入，如下图所示

<div style="width:75%; margin-left:auto; margin-right:auto; margin-bottom:8px; margin-top:8px;">
<img src="https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/RLNNFig6.png" alt="" >
</div>

该网络的输出向量是$$m=10$$个神经网络的平均值。该体系结构的选择是基于100次模拟运行中表现出最小`MSE`的结构，其性能会在下文简要分析。每个神经网络由两个全连接层组成，同样只有`weight`没有`bias`。隐藏层包含20个神经元（因此是160个参数，$$7\times 20+20\times 1=160$$），同样采用`log-sigmoid`传递函数进行激活，输出层是线性函数。基本参数实际上和前一个网络别无二致，只是结构稍加改变。

决定`NN2`输入的逻辑（即多目标性能值）十分复杂，需要很多条件语句。下面的算法1(`Algorithm 1`)描述了混合结构（即`RLNN2`）的一般操作过程，包括强化学习部分以及全连接网络之间的交互。`Algprithm 2`描述了`NN2`的输入选择逻辑，这是`Algorithm 1`中所必需的。所需的参数均在第`IV`部分进行定义和解释。

`2019-2-10 23:40:48`

说的什么几把垃圾

## IV - 结果 - RESULTS







## V - 结论 - CONCLUSIONS

## EX - 鸣谢

- 红豆泥感谢[@GeneZC](https://github.com/GeneZC)佬，我不会的问题都能帮我解决，隔着屏幕就感觉到了一股大佬的气息！

