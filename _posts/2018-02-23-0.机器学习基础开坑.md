---
layout: post
title:  "0.机器学习基础开坑"
date:   2018-02-23 21:01:13 +0000
categories: git push
comments: true
---
<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

现在已经把评论系统改到墙里了，开箱即用，只需要登录GitHub账号即可。

---
## 丑话说在前面（并不

在写这篇文章的时候，我个人已经看了一本叫做[《深度学习（Deep Learning）》][DeepLearning]的书的前一部分（链接给出的是中文的版本，英文版本也可以在GitHub中搜索）。这本书也被人叫做**“花书”**，对这方面的知识讲解也十分全面。前面作为基础知识的回顾复习也十分有效。

在看之前确保你已经对《线性代数》、《概率论》、《信息论》、《数值分析》的基础内容有一定的了解。
当然我自己对部分内容也只是略懂一些基础，这个日志实际上也只是学习笔记之类的内容，如果有错误请在评论区指出，十分感谢。

那么我就不从数学内容开始写了。直接从机器学习的相关内容开始。

---

深度学习是机器学习的一个分支。要学习深度学习，就要登高自卑酱紫。

当然在每篇日志后面都会有一个代码链接，这些代码是理解本次内容所必需的，一般的这些代码都是**Python**在**TensorFlow**框架下完成的，**有时间的话**我也会留下一篇有关环境配置的说明。

[Tom M. Mitchell][TomM.Mitchell](1997)给出的机器学习的定义是
>A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.


有一段中文翻译是
>对于某类任务T和性能度量，一个计算机程序如果通过经验E改进后，他在任务T上由性能度量P所衡量的性能会有所提升

实际上上面所说的E、T、P等定义十分广，在这里同样不会具体解释，但是会举出部分例子来说明。

如果简单程序都能解决的问题，机器学习不学习都不能做的更好，那么就不需要机器学习来完成这样的任务，所以机器学习所应付的问题往往是**简单程序很难解决的问题**（就像很多教程里面都会出现的mnist手写数字识别那样的问题，当然本文也避免不了解释这一段码←_←）一般的，机器学习会处理以下这类问题（上面所谓的T）

- 分类
通过对样本的分析，返回这个样本中每一个元素分别属于哪个分类
- 输入缺失的分类
比上面那个分类还复杂的分类（目前我还没做过）
- 回归
给定输入并预测数值，十分常用（比如这次的美国大学生数学建模竞赛中我们组采用的算法，虽然在本次的极小样本下表现能力极差，输出很乱。。。）
- 转录
（大概是语音识别中经常会使用的内容？）
- 翻译
谷歌现在的翻译水平很高，很大一部分原因就是改良了传统的机器翻译算法（如果我更得到RNN的话大概会说到吧）
- 结构化输出
书中给的例子是**像素级图像分割、自然语言语法树生成**等
- 异常检测
信用卡欺诈，垃圾邮件
- 合成和采样
这个可以重点看看生成式对抗神经网络（GAN）
- 缺失值填补
- 去噪
根据有噪声的样本去预测没有噪声的样本
- 密度估计（概率质量函数估计）

没有解释的内容是因为我也不了解（还要学习一个）

那么如何判断学习结果呢，上面说到性能度量P。度量一个模型的性能一般会采用准确率或者错误率，就像中小学生老师通过作业、测验成绩判断学生的学习质量一样。

作业、测验在这里就是所谓的测试数据集。但确实就像学生一样，老师通过测验得到的结果往往并不代表学生的真实水平，而对学生的真实的、综合的素质评价是很难的，就像这里，想设计一个完美的评价体系也是很难的。

有关如何选择性能度量P，在上面说的那本[《深度学习（Deep Learning）》][DeepLearning]中说的十分详细。由于本篇只是一个简介，更多内容不继续展开。这一系列的重点应该是深度学习的一些知识。

常说的**监督学习**(supervised)和**无监督学习**(unsupervised)的算法往往就是根据经验进行分类的。对于监督学习算法，我们经常会训练含有很多特征的数据集，每个数据集都会有**标签**（label），有时也叫**目标**（target）。手写数字的mnist数据集就在标签上写清了每一个图像所对应的真实数字，这个就是标签。算法通过对数据集的学习，最终可以在错误率极低的条件下识别绝大多数手写数字。而对于无监督学习算法，我们就会让算法去学习数据集上有用的性质。比如聚类等算法的机器学习实现。

当然这两个并不明确的定义也表明许多技术在两者间是共同的。对于表面上的无监督学习问题，我们一般可以分解为多个监督学习问题，而对于监督学习任务也可以由无监督学习完成。给出的《深度学习》参考书的5.1.3中就有详细举例。


和其他的地方一样，这里也采用一个**线性回归**的例子来说明一下上面说的内容。线性回归时，我们希望建立一个系统，使得对于输入的$$x$$和$$y$$，最大化预测x与y的线性关系，然后根据输入的其他$$x$$，预测输出$$y$$。

简单地说线性回归的关系就是

$$\widehat{y}=w^{T}x$$

至于为什么是这样的，大概之后会专门说一下。其中$$\widehat{y}$$代表利用模型预测的结果，这里的$$w^T$$和$$x$$是向量，$$w^T$$又经常被称为是参数，是本任务的目标。但是我们要明白，输入的$$x$$和$$y$$并不一定是一维的。多线性回归的本质和单线性回归是一样的。这个时候任务**T**就是根据预测的$$x$$和$$y$$关系来从$$x$$预测$$y$$，对于性能度量**P**，为了确保能尽可能评价模型性能，我们常采用**均方差**函数来评价。如果用于测试模型准确性的数据集是测试数据集，共有$$m$$组数据，用$$test$$表示，那么

$$MSE_{test}=\frac{1}{m}\sum_{i}(\widehat{y}^{test}-y^{test})_{i}^{2}$$

由于$$y$$的本质是多维向量，这种方式实际上可以直接写成2-范数的形式

$$MSE_{test}=\frac{1}{m}||\widehat{y}^{test}-y^{test}||_{2}^{2}$$

>对于范数，在向量中使用最多的是p-范数，而p-范数的表示形式如下
$$\left \| x \right \|_{p}=(|x_{1}|^{p}+|x_{2}|^{p}+...+|x_{n}|^{p})^{\frac{1}{p}}$$

2-范数一般用于表示向量之间的抽象的距离。这里用意是使得预测结果和样本结果的“距离”平方和最小。

为了让程序获得经验**E**，我们需要设计一个适合机器学习的算法，使得模型能够在测试样本中获取经验，从而减少MSE，获得最精确的模型。优化模型使MSE这个数值最小的算法经常采用**梯度下降法**，后文中会对这个算法进行说明，此处采用一个更加简单的算法，即**正规方程**。如果对吴恩达的机器学习课程有接触，对这个应该不陌生。最小化MSE的简单方法就是直接求导，使

$$\triangledown _{w}MSE_{train}=0$$

而在2-范数的形式下原式也很容易求导，令导数为0，可以得到

$$\triangledown _{w}\left ( Xw-y \right )^{T}\left ( Xw-y \right )=0$$

将矩阵乘开，可以得到

$$\triangledown _{w}\left ( w^{T}X^{T}Xw-2w^{T}X^{T}y+y^{T}y \right )=0$$

这里需要一些矩阵分析的知识，求导后得到的结果是

$$2X^{T}Xw-2X^{T}y=0$$

把$$w$$表示出来就是

$$w=\left ( X^{(train)T}X^{\left ( train \right )}\right )^{-1}X^{(train)T}y^{train}$$

则这个计算式就成为了一个简单的机器学习算法，上面的式子被称作正规方程。
但是我想了很久，感觉这个东西很难和现在的许多机器学习的算法联系起来。机器在这个过程中似乎并没有做什么，这个算法也只是一个传统算法罢了。有关此算法的代码如下，可以通过调整生成数据的数量观察效率（不过似乎效率蛮高的

{% highlight python %}
import numpy as np
from matplotlib import pyplot as plt
w = np.random.random((1, 1))
xlist = []
ylist = []
for i in range(100):
    noise = np.random.normal()
    x = np.random.random((1, 1)) * 100
    y = np.dot(w.T, x)
    xlist.append(x[0])
    ylist.append(y[0] + noise)
plt.plot(xlist, ylist, 'ro')
plt.show()
xlist, ylist = np.mat(xlist), np.mat(ylist)
w1 = np.dot(np.dot(np.dot(xlist.T, xlist).I, xlist.T), ylist)
print(w1, w)
{% endhighlight %}


对于数量比较小的样本时，人们通常采用正规方程的方式降低预测结果的误差，而对于数量较大（比如超过10000）的样本时，往往会采用梯度下降法进行计算。梯度下降法以后也会单独说明。

上述的模型其实也只是线性回归的一种特例，实际上线性回归时经常会出现另一个参数**偏置**(bias)，我们常写作$$b$$，即

$$\widehat{y}=w^{T}x+b$$

相应的计算过程也会稍显复杂一些。但这只作为一个最简单的例子来说明什么是机器学习。看到这里还不明白的话也没有关系，因为稍微复杂一点的实例都和这种例子有巨大的不同。当然在那些算法开始之前，还需要对一些术语有一定的了解。

[DeepLearning]:https://github.com/exacity/deeplearningbook-chinese
[TomM.Mitchell]:https://en.wikipedia.org/wiki/Machine_learning
{% if page.comments %}
<div id="container"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
var gitment = new Gitment({
  id: '0', // 可选。默认为 location.href
  owner: 'psycholsc',
  repo: 'temp',
  oauth: {
    client_id: '9183e7259ea6d850a7df',
    client_secret: 'd0a82473ca685629b50ded0553f402b6ba2b2dee',
  },
})
gitment.render('container')
</script>
{% endif %}